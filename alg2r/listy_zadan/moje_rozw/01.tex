\documentclass{article}

\usepackage{../../../notatki}

\title{\large Algebra 2R\smallskip\\ \textbf{Problem List 1}}
\author{\normalsize Weronika Jakimowicz}
\date{~~~}

\begin{document}
\maketitle
\thispagestyle{empty}

\subsection*{EXERCISE 1.}
\emph{Proof that $\C=\R[z]$ for every complex number $z\in \C\setminus\R$.}
\smallskip

To begin with, let us take any $z\in\C\setminus\R$ such that $z=ai$ for some $a\in\R$. We have that
$$\R[z]=\{f(z)\;:\;f\in\R[X]\}.$$

Let $I=(X^2+a^2)\normalsubgroup\R[X]$ be an ideal of $\R[X]$ generated by a polynomial with no real roots. We know that $\R[X]/I\cong \C$. 

This is because $\R$ is a field and so $\R[X]$ is an euclidean domain: if we take any $f\in\R[X]$ then we can write it as $f=v(X^2+a^2)+w$, where $w$ is of degree $0$ or $1$ ($<def(X^2+a^2)$) and so $f$ in $\R[X]/I$ is represented only by $w$. Now it is quite easy to map polynomials with real coefficients and maximal degree $1$ to $\C$, for example $f:\R[X]/I\to\C$ such that $f(aX+b)=ai+b$. Therefore $\R[X]/I\cong \C$.

Consider the evaluation homomorphism $\phi_z$ which maps $\R[X]\ni w\mapsto w(z)\in\R[z]$. We can see that $\ker(\phi_z)=(X^2+a^2)=I$. Therefore, by the fundamental theorem on ring homomorphism we have an isomorphism
$$f:Im(\phi_z)=\R[z]\to \R[X]/ker(\phi_z)=\R[X]/I$$
and as mentioned above, $\R[X]/I\cong\C$. Hence, $\R[z]\cong\C$.

\proofend

\subsection*{EXERCISE 2.}
\emph{\color{pink}Assume that $K\subset L$ are fields and $a,b\in L$. For a rational function $f(X)\in K(X)$ define $f(a)$ as ${g(a)\over h(a)}$, where $g,h\in K[X]$, $f=\frac gh$ and $h(a)\neq 0$, provided such $g,h$ exist. If not, $f(a)$ is undetermined. Prove that}
\smallskip

\emph{\color{pink}(a) if $f(X)\in K(X)$ and $f(a)$ is defined, then $f(a)$ is determined uniquely (does not depend on the choice of $g, h$)}
\smallskip

% Let $g,h\in K[X], h(a)\neq0$ be polynomials that have no common roots and $f={g\over h}$. This means that the quotient ${g\over h}$ cannot be simplified. Now, let us take another pair of polynomials $g',h'\in K[X], h'(a)\neq 0$ such that $f={g'\over h'}$. I claim that 
% $${g(a)\over h(a)}={g'(a)\over h'(a)}.$$

% We know that $f={g\over h}={g'\over h'}$ and hence
% $$gh'=hg'$$

%Because $K$ is a field, I know that $K[X]$ is an euclidean domain. 

%such that $g,h$ have no common divisor. This means that the quotient ${g\over h}$ cannot be simplified further. Because $K[X]$ is an euclidean domain, I can write $g=p\cdot h+r$, where $deg(r)<deg(h)$ and $r\neq 0$.

Suppose by contradiction that $f(a)$ depends on which $g,h$ we choose. That means that there exist $g,h, g',h'\in K[X], h(a)\neq 0, h'(a)$ such that $f={g\over h}={g'\over h'}$ but ${g(a)\over h(a)}+c= {g'(a)\over h'(a)}$, where $c\in L\setminus\{0\}$. 

From $f={g\over h}={g'\over h'}$ we get that $g\cdot h'=g'\cdot h$ and in particular
\begin{align*}
    (gh')(a)&=(g'h)(a)\\
    g(a)h'(a)&=g'(a)h(a)\\
    g(a)h'(a)-g'(a)h(a)=0
\end{align*}

From the assumption that $f(a)$ depends on the choice of polynomials we get that 
\begin{align*}
    {g'(a)\over h'(a)}&={g(a)\over h(a)}+c\\
    g'(a)h(a)&=g(a)h'(a)+ch'(a)\\
    g'(a)h(a)-g(a)h'(a)&=ch'(a)\neq 0
\end{align*}
Which is a contradiction because $c\neq 0$, $h'(a)\neq 0$ and we are working in a field that is a ring without zero divisors.
\smallskip

%$$f(a)={g(a)\over h(a)}={p(a)\cdot h(a)+r(a)\over h(a)}=p(a)+{r(a)\over h(a)}$$

\emph{\color{pink}(b) $K(a)=\{f(a)\;:\;f\in K(X)\text{ i }f(a)\text{ jest okreÅ›lone}\}$}

We know that $K(a)$ is a subfield of $L$ that is generated by $K\cup\{a\}$. Let us label this field as $L'$. We will show that $L'=K(a)$.

$L'\subseteq K(a)$

Let us take any $x\in L'$. Then $x$ is a finite linear combination of elements from $K$ and $\{a. a^{-1}\}$:
$$x=\sum\limits_{0\leq k\leq n}\alpha_k a^{i_kk},\quad i_k\in\{1,-1\},\;\alpha_k\in K.$$
We need to change this into a rational function. Take $p_k\in K[X]$ such that $p_k(X)=\alpha_kX^k$. We have that
$$x=\sum\limits_{0\leq k\leq n}p_k(a^{i_k}).$$
It is clear that when working with rational functions we may say that $p_k(a^{-1})={1\over p_k'(a)}$ where $p_k(X)=\alpha_k^{-1}X^k$.
\begin{align*}
    x&=\sum\limits_{0\leq k\leq n}p_k(a^{i_k})={\sum\limits_{0\leq k\leq n}p_k(a)\prod\limits_{\substack{0\leq l\leq n,\\ i_l=-1}}p_k'(a)\over\prod\limits_{\substack{0\leq k\leq n\\i_k=-1}}p_k'(a)}\in K(a)
\end{align*}

% a + b^-1 + c + d^-1 = a + 1/b + c + 1/d = ab/b + 1/b + cd/d + 1/d = abd/bd + d/bd + cdb/bd + b/bd = (abd+d+cdb+b)/bd

$K(a)\subseteq L'$

Let us take any $f\in K(X)$ such that $f(a)$ is defined. We may write $f={g\over h}$ for $g,h\in K[X]$ and $h(a)\neq 0$. We have that $g(a)\in L'$ and $h(a)\in L'$. Therefore, ${g(a)\over h(a)}=g(a)\cdot[h(a)]^{-1}\in L'$.
\smallskip

\emph{\color{pink}(c) $K(a, b)=(K(a))(b)$}
\smallskip



\subsection*{EXERCISE 3.}
\emph{Assume that $K\subseteq L$ are fields and $f_1,...,f_m\in K[X_1,...,X_n]$ have degree $1$.}

\emph{(a) Prove that if the system of equations $f_1=...=f_m=0$ has a solution in $L$ then it has a solution in $K$. (hint: use linear algebra).}
\smallskip

Let
$$f_i=\sum\limits_{1\leq k\leq n}b_{i, k}X_k$$
for $i=1,...,m$.

% Take $\overline a=(a_1,..., a_n)$ be a solution from $L$. We have
% $$0=f_i(\overline a)=\sum\limits_{1\leq k\leq n}b_{i, k}a_k.$$
% This is a linear combination of elements from $L$ and therefore we have three possibilities:

% \indent 1. $(\forall\;k=1,...,n)\;b_{i,k}=0$ and so this equation does not influence the remaining $(m-1)$ polynomials. From those remaining polynomials either one has non-zero coefficients $b_{j, k}$ (in this case we jump to case 2 or 3) or all polynomials from the set of equations are trivial and any sequence from $K$ is a solution.

% \indent 2. $(\forall\;k=1,...,n)\;a_k=0$ and hence $\overline a=(0,...,0)\in K^n$ is a solution.

% \indent 3. $a_k$ and $b_{i, k}$ are linearly dependent and
% \begin{align*}
%     0&=\sum\limits_{1\leq k\leq n}a_kb_{i, k}\\
%     0&=a_1b_{i, 1}+\sum\limits_{2\leq k\leq n}a_kb_{i, k}\\
%     -a_1b_{i, 1}&=\sum\limits_{2\leq k\leq n}a_kb_{i, k}\\
%     b_{i, 1}&=\sum\limits_{2\leq k\leq n}[a_k(-a_1)^{-1}]b_{i, k}
% \end{align*}
% The last operation is permitted because we are inside a field and $-a_1$ is non-zero, therefore it has a multiplicative inverse. We have that 
% $$\sum\limits_{2\leq k\leq n}[a_k(-a_1)^{-1}]b_{i, k}\in K$$
% and so $a_k(-a_1)^{-1}\in K\implies a_1,a_k\in K$.

% \proofend

We are working on linear equations, therefore we can construct a matrix that stores the same information as the system of equations $f_1=...=f_m$. Let
$$f_i=\sum\limits_{1\leq k\leq n}b_{i, k}X_k$$
for $i=1,...,m$. The matrix representation of this system of equations is:
$$\begin{bmatrix}
    b_{1,1} & b_{1, 2} & b_{1, 3} &... &b_{1, n-1} & b_{1, n}\\
    b_{2,1} & b_{2, 2} & b_{2, 3} &... &b_{2, n-1} & b_{2, n}\\
    ...     &   ...    & ...      &... & ...       & ...\\
    b_{m,1} & b_{m, 2} & b_{m, 3} &... &b_{m, n-1} & b_{1, n}
\end{bmatrix}X=0.$$
Using Gaussian algorithm, we can create an upper triangular matrix with coefficients from $K$. The solution would be found by backwards substitution. That is, $a_n$ would be in the bottom right corner of the matrix and it is an element of $K$ because such are the coefficients within my matrix. Then $a_{n-1}$ would be a combination of $a_n$ with two elements of $K$, hence it would still be in $K$ and so on. Each $a_i$ would be a linear combination of elements from $K$ and $a_k$, $k<i$, which we know are in $K$.

\proofend

\subsection*{EXERCISE 8.}
\emph{Prove that the set $\{\sqrt{p}\;:\;p\text{ is a prime number}\}$ is linearly independent over the field $\Q$.}
\smallskip

%A set $X$ is linearly independent over a field $K$ if $\sum\limits_{x\in X}\;a_xx=0$ for some $a_x\in K$ implies that $(\forall\;x)\;a_x=0$.

Assume that the set $S=\{\sqrt{p}\;:\;p\text{ is a prime number}\}$ is not linearly independent. That means that there is a sequence $p_1,...,p_n$ of prime numbers and $a_1,...,a_n\in\Q$ such that
$$\sum\limits_{1\leq k\leq n}a_k\sqrt{p_j}=0$$

\end{document}